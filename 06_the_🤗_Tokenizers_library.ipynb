{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_the_ðŸ¤—_Tokenizers_library.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbVY37YFte9PgwiApFDe4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/educatorsRlearners/hugging_face_course/blob/main/06_the_%F0%9F%A4%97_Tokenizers_library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "RDk09rwilDJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When fine-tuning a model, it only makes sense to use the same tokenizer that it was trained on. But what do you do when you want to create a model from scratch? \n",
        "\n",
        "Well, that's exactly what we're going to do in this chapter. \n",
        "\n",
        "# Training a new tokeinizer from an old one\n",
        "\n",
        "Key point: if a language model is not available in our target language or, and this is more likely in my case, the corpus is significantly different from the one a language model was trained on, then we're going to want to train a model from scratch using a tokenizer adapted to our data. \n",
        "\n",
        "For instance, if we want to tokenize a simple sentence like, \"I went shopping with my mother last week,\" the standard Bert-based tokenizer works well:"
      ],
      "metadata": {
        "id": "vn74WSQii3BK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jlho--6uibGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65777b4d-cc41-4a9b-b52d-c9e208f7cb3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'went', 'shopping', 'with', 'my', 'mother', 'last', 'week', '.']\n"
          ]
        }
      ],
      "source": [
        "checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sample_sentence = \"I went shopping with my mother last week.\"\n",
        "\n",
        "print(tokenizer.tokenize(sample_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, if we try to pass a highly technical, academic, or archaic text, then the results aren't nearly as good: "
      ],
      "metadata": {
        "id": "Knx8X7demnLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medical = \"the medical vocabulary is divided into many sub-token: paracetamol, pharyngitis, and oxycodone.\"\n",
        "\n",
        "print(tokenizer.tokenize(medical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuJmRrKLl7Ug",
        "outputId": "92307c9b-2dd1-487b-bc3d-586f77ebef10"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'medical', 'vocabulary', 'is', 'divided', 'into', 'many', 'sub', '-', 'token', ':', 'para', '##ce', '##tam', '##ol', ',', 'ph', '##ary', '##ng', '##itis', ',', 'and', 'ox', '##y', '##co', '##don', '##e', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To that end, training a tokenizer consists of four steps: \n",
        "- building a corpus\n",
        "- selecting the tokenizer architecture\n",
        "- training the tokenizer on the corpus\n",
        "- saving the result\n",
        "\n",
        "## [Assembling a corpus](https://huggingface.co/course/chapter6/2?fw=pt#assembling-a-corpus)"
      ],
      "metadata": {
        "id": "_-1BiBhNoc_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RuD12B7Qn8jF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}